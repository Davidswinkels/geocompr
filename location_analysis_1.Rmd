# Location analysis

## Prerequisites

```{r} 
library(sp)
library(raster)
library(osmdata)
library(tidyverse)
```

## Introduction
An ecological amplitude specifies a species' needs and optimum along different gradients.
For example, an icebear prefers northern latitudinal regions where the ice is persisent throughout the summer, where there is enough food (seals and sea lions) and where it can find shelter for its offspring.
The farther away an icebear is forced to live from these favorable conditions the more unlikely is its survival.
Similarily, a market product can have an economic niche.
There are locations of low and high turnovers.
And it is one task of geomarketing to identify regions where business is profitable.
Typical geomarketing questions include:
- Where are my clients?
- Where are my target groups?
- Can many customers easily reach my stores?
- Do my stores over-/underexploit the market potential?
- where are competitors?
- How big is my market share, and are there spatial differences?
- Are there regions with bigger turnovers?

## Use case

Suppose we would like to set up bike shops in all metropolitan areas of Germany.
We know that the majority of our clients are between 20-40 years old. 
Besides, our clientele is predominantly male, and lives alone or with just one person (single households, not families).

Procedure:
- download German census data and convert it into a raster (resolution: 1 km^2^)
- find metropolitan areas, i.e. aggregate to a resolution of 10 sqkm (define a threshold for metropolitan areas, and point out that there are still more advanced ways to do so)
- download OSM, find bike shops + other interesting POIs.
Create a POI raster (= attraction raster)
- restrict to metropolitan areas
- weights:
  - reclassify age raster: 16-40 weight = 2; all other weight = 1
  - reclassify gender raster:
  - reclassify household raster
	- POIs: reclassify into 5 classes, the more POIs, the better
	- finally map algebra: age + gender + POI raster will show the most favorable locations
- Exercise: download the 100 m census inhabitant data and convert it into a raster (will be used in the chapter continuing location analysis)
  
In an advanced chapter we could continue the analysis as follows:
 - restrict our analysis to Nuremberg (but point out that we could do the whole analysis simultaneously for all metropolitan areas in Germany)
 - choose optimal location based on number of inhabitants -> you should reach as much people within 15 minutes as possible (routing, catchment area)
 - the farther away we get from a shop, the more unlikely people will go to our shop (-> decay distance)
 - however, we have to take into account that there are already competitors (huff model, attraction) 

## Data download

Download the census data, unzip it and read it into R.
```{r, eval = FALSE}
url = paste0("https://www.zensus2011.de/SharedDocs/Downloads/DE/", 
             "Pressemitteilung/DemografischeGrunddaten/csv_Zensusatlas_", 
             "klassierte_Werte_1km_Gitter.zip?__blob=publicationFile&v=8")
download.file(url = url, destfile = file.path(tempdir(), "census.zip"),
              method = "auto", mode = "wb")
# list the file names
nms = unzip(file.path(tempdir(), "census.zip"), list = TRUE)
# unzip only the csv file
unzip(file.path(tempdir(), "census.zip"), files = nms$Name[2], exdir = tempdir())
# read in the csv file
input = readr::read_csv2(file.path(tempdir(), nms$Name[2]))
```

Next we would like to only select the variables we need. 
Additionally, we would like to set the values -1 and -9 to `NA` since these values are either unknown or secret.

```{r}
# only select the variables we need
# inh = inhabitants, hh_size = household size
input = dplyr::select(input, x = x_mp_1km, y = y_mp_1km, inh = Einwohner,
                      mean_age = Alter_D, women = Frauen_A, 
                      hh_size = HHGroesse_D)
# set -1 and -9 to NA since this corresponds to unknown or secret values
input = mutate_all(input, funs(ifelse(. %in% c(-1, -9), NA, .)))
```

After the data preprocessing, we can convert the table into a raster stack making use of the x- and y-coordinates.

```{r}
# convert table into a raster (x and y are cell midpoints)
coordinates(input) =~ x + y
# use the correct projection
proj4string(input) = CRS("+init=epsg:3035")
gridded(input) = TRUE
# convert into a raster stack
input = stack(input)
# plot(input)
```

Reclassify inhabitant data.

```{r}
rcl = matrix(c(1, 1, 125, 2, 2, 375, 3, 3, 1250, 4, 4, 3000, 5, 5, 6000,
               6, 6, 8000), ncol = 3, byrow = TRUE)
inh = reclassify(input$inh, rcl = rcl, right = NA)
```

Aggregate it to a resolution of 20000 sqkm.

```{r}
inh_agg = aggregate(inh, fact = 20000 / res(inh)[1], fun = sum)
plot(inh_agg > 500000)
polys = rasterToPolygons(inh_agg[inh_agg > 500000, drop = FALSE])
polys = st_as_sf(polys)
```

Plotting our polygons reveals eight metropolitan regions.
Each region consists of one ore more polygons (pixels).
It would be nice if we could join all polygons belonging to one region.
One approach is to dissolve all polygons.

```{r}
polys = summarize(polys, pop = sum(layer))
```

This returns one multipolygon feature with its elements corresponding to the metropolitan regions. 
To extract these polygons from the multipolygon, we can use `st_cast()`.

```{r}
polys = st_cast(polys, "POLYGON")
``` 

The warning message tells us that the attributes are repeated for all sub-geometries.
In our case, this means that each region receives the total population of all metropolitan areas combined.
Clearly this is wrong but here we can safely ignore this since we are merely interested in the geometry.

<!-- maybe a good if advanced exercise
This requires finding the nearest neighbors (`st_intersects()`), and some additional processing.
Do not worry too much about the following code.
There is probably a better way to do it. 
Nevertheless, it finds all pixels belonging to one region in a generic way.
We use this information to assign each polygon (pixel) to a region.
Subsequently, we can use the region information to dissolve the pixels into region polygons.

```{r}
# dissolve on spatial neighborhood
nbs = st_intersects(polys, polys)
# nbs = over(polys, polys, returnList = TRUE)

fun = function(x, y) {
  tmp = lapply(y, function(i) {
  if (any(x %in% i)) {
   union(x, i)
  } else {
   x
    }
  })
  Reduce(union, tmp)
}
# call function recursively
fun_2 = function(x, y) {
  out = fun(x, y)
  while (length(out) < length(fun(out, y))) {
    out = fun(out, y)
  }
  out
}

cluster = map(nbs, ~ fun_2(., nbs) %>% sort)
# just keep unique clusters
cluster = cluster[!duplicated(cluster)]
# assign the cluster classes to each pixel
for (i in seq_along(cluster)) {
  polys[cluster[[i]], "region_id"] = i
}
# dissolve pixels based on the the region id
polys = group_by(polys, region_id) %>%
  summarize(pop = sum(layer, na.rm = TRUE))
# polys_2 = aggregate(polys, list(polys$region_id), sum)
plot(polys[, "region_id"])

# Another approach, can be also be part of an excercise

coords = st_coordinates(polys_3) %>% 
  as.data.frame
ls = split(coords, f = coords$L2)
ls = lapply(ls, function(x) {
  dplyr::select(x, X, Y) %>%
    as.matrix %>%
    list %>%
    st_polygon
})
metros = do.call(st_sfc, ls)
metros = st_set_crs(metros, 3035)
metros = st_sf(data.frame(region_id = 1:9), geometry = metros)
st_intersects(metros, metros)
plot(metros[-5,])
st_centroid(metros) %>%
  st_coordinates
```
-->

This leaves us with a tiny problem.
There are only eight metropolitan areas whereas the dissolving left us with nine.
This is because one polygon just touches the corner of another polygon (western Germany, Cologne/Düsseldorf area).
We could assign it to the neighboring region using another dissolving procedure, however, we leave this as an execercise to the reader.
Here, we simply delete the polygon.

```{r}
# find out about the offending polygon
int = st_intersects(metros, metros)
# polygons 5 and 9 share one border, delete polygon number 5
metros = metros[-5, ]
```

Now that we have defined the metropolitan areas in which we want to place our bike shops, we additionally want to give them a name.
A reverse geocoding approach can help with that.
Given a coordinate, reverse gecoding finds the corresponding address.
Consequently, we need to extract the centroid coordinate of each metropolitan area.
The resulting coordinates can then serve as an input for a reverse geocoding API.
Here, we make use of the one provided by Google accessed through the **ggmap** package.
Note that Google lets you use its services on a free basis for a maximum of 2500 queries a day.
Since `ggmap::revgeocode()` only accepts lat/lon coordinates, we need to first reproject the metropolitan polygons.

```{r}
# reverse geocoding to find out about the metropolitan names
polys_2 = st_transform(polys, 4326)
coords = st_centroid(polys_2) %>%
  st_coordinates
metros = lapply(1:nrow(coords), function(i) {
  # give the server a bit time
  Sys.sleep(sample(seq(1.2, 3, 0.1), 1))
  ggmap::revgeocode(coords[i, ], output = "more")
})
metros = bind_rows(metros) %>%
  select(locality, administrative_area_level_2)
# replace Velbert
metros = mutate(metros, 
                locality = ifelse(locality == "Velbert",
                                  administrative_area_level_2, locality)) %>%
  select(locality) %>%
  pull
```

Munich, Stuttgart, Nuremberg, Frankfurt, Cologne/Düsseldorf, Hamburg, Leipzig, Berlin
(missing: Dresden, Bielefeld, Bremen, Hannover, but ok)

```{r}
library(osmdata)
ls_1 = lapply(metros, function(x) {
  # give the server a bit time
  Sys.sleep(10)
  q0 = opq(x)
  q1 = add_osm_feature(q0, key = 'shop')
  points = osmdata_sf(q1)
  points = dplyr::select(points, osm_points)
  points = st_set_crs(points, 4326)
  select(points, shop)
  })
# rbind all shops
shops = Reduce(rbind, ls_2)
shops$count = 1
shops = st_transform(shops, proj4string(input$inh))
shops = as(shops, "Spatial")
# create poi raster
poi = input$inh
poi[] = NA
poi = rasterize(x = shops, y = poi, field = "count", fun = "sum")
plot(poi, xlim = c(4380000, 4420000), ylim = c(2920000, 2940000))
```




Reference (Huff model): https://journal.r-project.org/archive/2017/RJ-2017-020/RJ-2017-020.pdf